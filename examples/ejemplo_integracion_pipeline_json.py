"""
Ejemplo de Integraci√≥n Mejorada: Logging JSON en Pipeline Principal
==================================================================

Este ejemplo demuestra la integraci√≥n completa del sistema de logging JSON
avanzado en el PipelineOrchestrator, proporcionando trazabilidad end-to-end
con formato estructurado para sistemas de monitoreo.
"""

import pandas as pd
import time
import json
from datetime import datetime
from typing import Dict, Any
from processing.json_utils import to_serializable  # Ajusta el import si la funci√≥n est√° en otro m√≥dulo

# Importar el PipelineOrchestrator mejorado
from orchestrator.pipeline_orchestrator import PipelineOrchestrator


def crear_datos_ejemplo() -> pd.DataFrame:
    """Crea datos de ejemplo para las pruebas del pipeline"""
    return pd.DataFrame({
        'edad': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],
        'genero': ['Masculino', 'Femenino', 'Masculino', 'Femenino', 'Masculino', 
                  'Femenino', 'Masculino', 'Femenino', 'Masculino', 'Femenino'],
        'nivel_educacion': ['Primaria', 'Secundaria', 'Universitaria', 'Postgrado', 'Doctorado',
                           'Primaria', 'Secundaria', 'Universitaria', 'Postgrado', 'Doctorado'],
        'ingresos': [500000, 800000, 1200000, 1500000, 2000000, 
                    600000, 900000, 1300000, 1600000, 2200000],
        'region': ['Metropolitana', 'Valpara√≠so', 'Antofagasta', 'Tarapac√°', 'Atacama',
                  'Metropolitana', 'Valpara√≠so', 'Antofagasta', 'Tarapac√°', 'Atacama'],
        'pregunta_1_likert': [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],
        'pregunta_2_likert': [2, 3, 4, 5, 1, 2, 3, 4, 5, 1]
    })


def ejemplo_pipeline_completo_json():
    """
    Ejemplo de ejecuci√≥n completa del pipeline con logging JSON integrado.
    """
    print("üöÄ EJEMPLO: PIPELINE COMPLETO CON LOGGING JSON INTEGRADO")
    print("="*80)
    
    # Crear datos de ejemplo y guardarlos
    df = crear_datos_ejemplo()
    file_path = "datos_ejemplo_pipeline.csv"
    df.to_csv(file_path, index=False)
    
    print(f"üìä Datos de ejemplo creados: {df.shape[0]} filas, {df.shape[1]} columnas")
    print(f"üíæ Archivo guardado: {file_path}")
    
    # Crear PipelineOrchestrator con logging JSON
    print(f"\nüîß Inicializando PipelineOrchestrator...")
    orchestrator = PipelineOrchestrator()
    
    print(f"üìù Session ID generado: {orchestrator.session_id}")
    print(f"üîç JSON Logger configurado: {orchestrator.json_logger.session_id}")
    
    # Ejecutar pipeline completo
    print(f"\nüîÑ Ejecutando pipeline completo...")
    
    start_time = time.time()
    
    success = orchestrator.run_full_pipeline(
        file_path=file_path,
        schema=None,  # Usar detecci√≥n autom√°tica
        filters=None  # Sin filtros
    )
    
    execution_time = time.time() - start_time
    
    print(f"\n‚úÖ Pipeline {'completado exitosamente' if success else 'fall√≥'}")
    print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {execution_time:.3f} segundos")
    
    # Obtener resumen completo
    summary = orchestrator.get_pipeline_summary()
    
    print(f"\nüìà RESUMEN DEL PIPELINE:")
    print(f"   - Session ID: {summary['session_id']}")
    print(f"   - Datos cargados: {summary['data_loaded']}")
    print(f"   - Forma de datos: {summary['data_shape']}")
    print(f"   - Reportes generados: {summary['reports_generated']}")
    
    # Resumen de logs JSON
    json_summary = summary['json_logs_summary']
    print(f"\nüìã RESUMEN DE LOGS JSON:")
    print(f"   - Total de logs: {json_summary['total_logs']}")
    print(f"   - Distribuci√≥n por nivel: {json_summary['level_distribution']}")
    print(f"   - Distribuci√≥n por categor√≠a: {json_summary['category_distribution']}")
    print(f"   - Errores: {json_summary['error_count']}")
    print(f"   - Tiempo total de logs: {json_summary['total_execution_time']:.3f}s")
    
    # Exportar resultados
    export_path = f"resultados_pipeline_{orchestrator.session_id}.json"
    orchestrator.export_results(export_path)
    
    print(f"\nüíæ Resultados exportados a: {export_path}")
    
    return orchestrator, success


def ejemplo_pipeline_paso_a_paso():
    """
    Ejemplo de ejecuci√≥n paso a paso del pipeline con logging detallado.
    """
    print("\n" + "="*80)
    print("EJEMPLO: PIPELINE PASO A PASO CON LOGGING DETALLADO")
    print("="*80)
    
    # Crear datos de ejemplo
    df = crear_datos_ejemplo()
    file_path = "datos_ejemplo_paso_a_paso.csv"
    df.to_csv(file_path, index=False)
    
    # Crear PipelineOrchestrator
    orchestrator = PipelineOrchestrator()
    print(f"üìù Pipeline inicializado con session_id: {orchestrator.session_id}")
    
    # Ejecutar pasos individualmente
    steps = [
        ("Carga de Datos", lambda: orchestrator.load_data(file_path)),
        ("Validaci√≥n de Esquema", lambda: orchestrator.validate_schema()),
        ("Clasificaci√≥n Sem√°ntica", lambda: orchestrator.classify_semantics()),
        ("Validaci√≥n de Reglas de Negocio", lambda: orchestrator.validate_business_rules_step()),
        ("Filtrado de Datos", lambda: orchestrator.apply_filters()),
        ("Ingenier√≠a de Caracter√≠sticas", lambda: orchestrator.run_feature_engineering()),
        ("An√°lisis Estad√≠stico", lambda: orchestrator.run_statistical_analysis()),
        ("Generaci√≥n de Visualizaciones", lambda: orchestrator.generate_visualizations()),
        ("Generaci√≥n de Reportes", lambda: orchestrator.generate_reports()),
    ]
    
    print(f"\nüîÑ Ejecutando pasos individualmente:")
    
    for i, (step_name, step_function) in enumerate(steps, 1):
        print(f"\n   {i}. {step_name}...")
        
        try:
            start_time = time.time()
            success = step_function()
            execution_time = time.time() - start_time
            
            status = "‚úÖ" if success else "‚ùå"
            print(f"      {status} Completado en {execution_time:.3f}s")
            
            if not success:
                print(f"      ‚ö†Ô∏è Paso fall√≥, deteniendo pipeline")
                break
                
        except Exception as e:
            print(f"      ‚ùå Error: {e}")
            break
    
    # Obtener logs espec√≠ficos por categor√≠a
    print(f"\nüìä AN√ÅLISIS DE LOGS POR CATEGOR√çA:")
    
    categories = ["data_load", "validation", "processing", "analysis", "visualization", "export"]
    
    for category in categories:
        logs = orchestrator.json_logger.get_logs_by_category(category)
        if logs:
            print(f"   - {category}: {len(logs)} logs")
            
            # Mostrar ejemplo de log
            example_log = logs[0]
            print(f"     Ejemplo: {example_log.message}")
    
    # Obtener logs de errores
    error_logs = orchestrator.json_logger.get_error_logs()
    if error_logs:
        print(f"\n‚ùå LOGS DE ERRORES ({len(error_logs)}):")
        for log in error_logs:
            print(f"   - {log.timestamp}: {log.message}")
            if log.error_details:
                print(f"     Detalles: {log.error_details}")
    
    return orchestrator


def ejemplo_multiple_pipelines():
    """
    Ejemplo de m√∫ltiples pipelines ejecut√°ndose con logging independiente.
    """
    print("\n" + "="*80)
    print("EJEMPLO: M√öLTIPLES PIPELINES CON LOGGING INDEPENDIENTE")
    print("="*80)
    
    from processing.json_logging import get_json_logger_manager
    
    # Crear gestor de logging
    manager = get_json_logger_manager()
    
    # Crear datos de ejemplo
    df = crear_datos_ejemplo()
    
    # Crear m√∫ltiples pipelines
    pipelines = []
    for i in range(3):
        # Crear archivo √∫nico para cada pipeline
        file_path = f"datos_pipeline_{i+1}.csv"
        df.to_csv(file_path, index=False)
        
        # Crear pipeline con session_id √∫nico
        session_id = f"pipeline_{i+1}_{datetime.now().strftime('%H%M%S')}"
        orchestrator = PipelineOrchestrator()
        orchestrator.session_id = session_id
        orchestrator.json_logger = manager.create_session(session_id)
        
        pipelines.append((orchestrator, file_path, f"Pipeline {i+1}"))
    
    print(f"üìù Pipelines creados: {len(pipelines)}")
    
    # Ejecutar pipelines en paralelo (simulado)
    results = []
    for orchestrator, file_path, pipeline_name in pipelines:
        print(f"\nüîÑ Ejecutando {pipeline_name}...")
        
        try:
            # Ejecutar solo algunos pasos para demostraci√≥n
            success1 = orchestrator.load_data(file_path)
            success2 = orchestrator.validate_schema()
            success3 = orchestrator.validate_business_rules_step()
            
            success = all([success1, success2, success3])
            results.append((pipeline_name, success))
            
            print(f"   ‚úÖ {pipeline_name}: {'Exitoso' if success else 'Fall√≥'}")
            
        except Exception as e:
            print(f"   ‚ùå {pipeline_name}: Error - {e}")
            results.append((pipeline_name, False))
    
    # Obtener resumen de todas las sesiones
    all_summaries = manager.get_all_sessions_summary()
    
    print(f"\nüìä RESUMEN DE TODAS LAS SESIONES:")
    print(f"   - Total de sesiones: {all_summaries['total_sessions']}")
    print(f"   - Sesiones activas: {len(all_summaries['active_sessions'])}")
    
    for session_id, summary in all_summaries['session_summaries'].items():
        print(f"\n   üìà Sesi√≥n {session_id}:")
        print(f"      - Logs: {summary['total_logs']}")
        print(f"      - Errores: {summary['error_count']}")
        print(f"      - Tiempo total: {summary['total_execution_time']:.3f}s")
    
    # Exportar todas las sesiones
    export_path = manager.export_all_sessions()
    print(f"\nüíæ Todas las sesiones exportadas a: {export_path}")
    
    return manager, results


def ejemplo_formato_json_generado():
    """
    Ejemplo que muestra el formato JSON generado por el pipeline.
    """
    print("\n" + "="*80)
    print("EJEMPLO: FORMATO JSON GENERADO POR EL PIPELINE")
    print("="*80)
    
    # Crear pipeline
    orchestrator = PipelineOrchestrator()
    
    # Crear datos de ejemplo
    df = crear_datos_ejemplo()
    file_path = "datos_ejemplo_formato.csv"
    df.to_csv(file_path, index=False)
    
    # Ejecutar solo el primer paso para obtener logs
    orchestrator.load_data(file_path)
    
    # Obtener logs de la sesi√≥n
    logs = orchestrator.json_logger.get_session_logs()
    
    print(f"üìÑ FORMATO JSON GENERADO:")
    print(f"   - Total de logs: {len(logs)}")
    
    if logs:
        # Mostrar el primer log como ejemplo
        example_log = logs[0]
        print(f"\nüîç EJEMPLO DE LOG JSON:")
        print(json.dumps(example_log.to_dict(), indent=2, ensure_ascii=False, default=to_serializable))
        
        # Mostrar campos espec√≠ficos
        print(f"\nüìã CAMPOS CLAVE:")
        print(f"   - Timestamp: {example_log.timestamp}")
        print(f"   - Session ID: {example_log.session_id}")
        print(f"   - Level: {example_log.level}")
        print(f"   - Category: {example_log.category}")
        print(f"   - Module: {example_log.module}")
        print(f"   - Function: {example_log.function}")
        print(f"   - Step: {example_log.step}")
        print(f"   - Execution Time: {example_log.execution_time}s")
        print(f"   - Tags: {example_log.tags}")
        
        # Mostrar m√©tricas
        print(f"\nüìä M√âTRICAS:")
        print(f"   - Before: {example_log.before_metrics}")
        print(f"   - After: {example_log.after_metrics}")
        
        # Mostrar informaci√≥n del sistema
        if example_log.system_info:
            print(f"\nüíª INFORMACI√ìN DEL SISTEMA:")
            print(f"   - Platform: {example_log.system_info.platform}")
            print(f"   - Python: {example_log.system_info.python_version}")
            print(f"   - CPU Count: {example_log.system_info.cpu_count}")
            print(f"   - Memory: {example_log.system_info.memory_total / (1024**3):.1f} GB")
            print(f"   - Process ID: {example_log.system_info.process_id}")
    
    return orchestrator


def ejemplo_integracion_con_sistemas_monitoreo():
    """
    Ejemplo de configuraci√≥n para integraci√≥n con sistemas de monitoreo.
    """
    print("\n" + "="*80)
    print("EJEMPLO: INTEGRACI√ìN CON SISTEMAS DE MONITOREO")
    print("="*80)
    
    # Configuraciones para diferentes sistemas
    configs = {
        "elk_stack": {
            "json_logging": {
                "enabled": True,
                "json_file": "logs/elk_pipeline.json",
                "console_json": True,
                "monitoring": {
                    "elk": {
                        "enabled": True,
                        "host": "localhost",
                        "port": 9200,
                        "index_prefix": "pipeline-logs"
                    }
                }
            }
        },
        "datadog": {
            "json_logging": {
                "enabled": True,
                "json_file": "logs/datadog_pipeline.json",
                "console_json": True,
                "monitoring": {
                    "datadog": {
                        "enabled": True,
                        "api_key": "your_api_key_here",
                        "host": "localhost",
                        "port": 8126
                    }
                }
            }
        },
        "prometheus": {
            "json_logging": {
                "enabled": True,
                "json_file": "logs/prometheus_pipeline.json",
                "console_json": True,
                "monitoring": {
                    "prometheus": {
                        "enabled": True,
                        "port": 9090,
                        "metrics_path": "/metrics"
                    }
                }
            }
        }
    }
    
    print("üîß CONFIGURACIONES PARA SISTEMAS DE MONITOREO:")
    
    for system, config in configs.items():
        print(f"\nüìä {system.upper()}:")
        print(f"   - Archivo JSON: {config['json_logging']['json_file']}")
        print(f"   - Console JSON: {config['json_logging']['console_json']}")
        
        monitoring_config = config['json_logging']['monitoring']
        for monitor, monitor_config in monitoring_config.items():
            if monitor_config.get('enabled', False):
                print(f"   - {monitor}: Habilitado")
                if 'host' in monitor_config:
                    print(f"     Host: {monitor_config['host']}:{monitor_config.get('port', 'default')}")
    
    print(f"\nüí° VENTAJAS DE LA INTEGRACI√ìN:")
    print(f"   ‚úÖ Formato JSON estandarizado para f√°cil ingesti√≥n")
    print(f"   ‚úÖ Session ID √∫nico para trazabilidad end-to-end")
    print(f"   ‚úÖ M√©tricas detalladas de rendimiento")
    print(f"   ‚úÖ Compatibilidad con ELK, Datadog, Prometheus")
    print(f"   ‚úÖ An√°lisis de errores y debugging")
    print(f"   ‚úÖ Auditor√≠a completa del pipeline")
    
    print(f"\nüîß PR√ìXIMOS PASOS PARA INTEGRACI√ìN:")
    print(f"   1. Configurar config.yml con la configuraci√≥n deseada")
    print(f"   2. Instalar y configurar Filebeat para ELK")
    print(f"   3. Configurar Datadog Agent para recolecci√≥n")
    print(f"   4. Configurar Prometheus para m√©tricas")
    print(f"   5. Crear dashboards y alertas")


def main():
    """
    Funci√≥n principal que ejecuta todos los ejemplos de integraci√≥n.
    """
    print("üöÄ EJEMPLO DE INTEGRACI√ìN MEJORADA: LOGGING JSON EN PIPELINE")
    print("="*80)
    
    try:
        # Ejemplo 1: Pipeline completo
        orchestrator1, success1 = ejemplo_pipeline_completo_json()
        
        # Ejemplo 2: Pipeline paso a paso
        orchestrator2 = ejemplo_pipeline_paso_a_paso()
        
        # Ejemplo 3: M√∫ltiples pipelines
        manager, results = ejemplo_multiple_pipelines()
        
        # Ejemplo 4: Formato JSON
        orchestrator3 = ejemplo_formato_json_generado()
        
        # Ejemplo 5: Integraci√≥n con sistemas de monitoreo
        ejemplo_integracion_con_sistemas_monitoreo()
        
        print("\n" + "="*80)
        print("‚úÖ TODOS LOS EJEMPLOS COMPLETADOS EXITOSAMENTE")
        print("="*80)
        
        print(f"\nüéØ BENEFICIOS LOGRADOS:")
        print(f"   ‚úÖ Trazabilidad completa con session_id √∫nico")
        print(f"   ‚úÖ Logs estructurados en formato JSON")
        print(f"   ‚úÖ M√©tricas detalladas de cada paso")
        print(f"   ‚úÖ Integraci√≥n con sistemas de monitoreo")
        print(f"   ‚úÖ An√°lisis de errores y debugging")
        print(f"   ‚úÖ Auditor√≠a completa del pipeline")
        
        print(f"\nüìö ARCHIVOS IMPORTANTES:")
        print(f"   - orchestrator/pipeline_orchestrator.py - Pipeline con JSON logging")
        print(f"   - processing/json_logging.py - Sistema JSON avanzado")
        print(f"   - config/config.yml - Configuraci√≥n de logging")
        print(f"   - logs/pipeline.json - Logs JSON generados")
        
        print(f"\nüîß INTEGRACI√ìN COMPLETADA:")
        print(f"   El PipelineOrchestrator ahora incluye logging JSON avanzado")
        print(f"   Cada paso del pipeline genera logs estructurados")
        print(f"   Session ID √∫nico para trazabilidad end-to-end")
        print(f"   Compatible con sistemas de monitoreo (ELK, Datadog, Prometheus)")
        
    except Exception as e:
        print(f"\n‚ùå Error en la ejecuci√≥n: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 