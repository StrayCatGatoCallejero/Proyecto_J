# ğŸ§ª Tests Unificados - Proyecto J

## ğŸ“‹ DescripciÃ³n

Esta carpeta contiene todos los tests unificados del proyecto, organizados de manera estructurada y siguiendo las mejores prÃ¡cticas de testing.

## ğŸ“ Estructura

```
tests/
â”œâ”€â”€ unit/                          # ğŸ§ª Tests unitarios
â”‚   â”œâ”€â”€ test_imports.py           # âœ… Importaciones del proyecto
â”‚   â”œâ”€â”€ test_imports_legacy.py    # ğŸ“œ Tests legacy de importaciones
â”‚   â”œâ”€â”€ test_excel_load_legacy.py # ğŸ“œ Tests legacy de carga Excel
â”‚   â””â”€â”€ test_variable_classifier_legacy.py # ğŸ“œ Tests legacy de clasificador
â”œâ”€â”€ integration/                   # ğŸ”— Tests de integraciÃ³n
â”‚   â”œâ”€â”€ test_pipeline.py          # âœ… Pipeline completo
â”‚   â”œâ”€â”€ test_complex_grouping_legacy.py # ğŸ“œ Tests legacy de agrupaciones
â”‚   â”œâ”€â”€ test_nl_query_legacy.py   # ğŸ“œ Tests legacy de consultas NL
â”‚   â””â”€â”€ test_streamlit_fix_legacy.py # ğŸ“œ Tests legacy de Streamlit
â”œâ”€â”€ e2e/                          # ğŸŒ Tests end-to-end
â”‚   â””â”€â”€ test_app_workflow.py      # âœ… Flujo completo de aplicaciÃ³n
â”œâ”€â”€ fixtures/                      # ğŸ“¦ Datos de prueba
â”‚   â”œâ”€â”€ sample_data.csv           # âœ… Datos de ejemplo
â”‚   â”œâ”€â”€ test_config.yml           # âœ… ConfiguraciÃ³n de prueba
â”‚   â””â”€â”€ expected_outputs/         # âœ… Salidas esperadas
â”œâ”€â”€ conftest.py                   # âš™ï¸ ConfiguraciÃ³n pytest
â”œâ”€â”€ pytest.ini                   # âš™ï¸ ConfiguraciÃ³n pytest
â”œâ”€â”€ run_tests.py                  # ğŸš€ Script de ejecuciÃ³n
â””â”€â”€ README.md                     # ğŸ“š Este archivo
```

## ğŸš€ EjecuciÃ³n de Tests

### EjecuciÃ³n BÃ¡sica

```bash
# Ejecutar todos los tests
python tests/run_tests.py

# Ejecutar solo tests unitarios
python tests/run_tests.py --type unit

# Ejecutar solo tests de integraciÃ³n
python tests/run_tests.py --type integration

# Ejecutar solo tests E2E
python tests/run_tests.py --type e2e
```

### EjecuciÃ³n Avanzada

```bash
# Con cobertura de cÃ³digo
python tests/run_tests.py --coverage

# Con reporte HTML
python tests/run_tests.py --html-report

# En modo verbose
python tests/run_tests.py --verbose

# En paralelo
python tests/run_tests.py --parallel

# Limpiar archivos temporales antes de ejecutar
python tests/run_tests.py --clean
```

### EjecuciÃ³n Directa con pytest

```bash
# Ejecutar tests especÃ­ficos
pytest tests/unit/test_imports.py -v

# Ejecutar tests con marcadores
pytest -m unit -v
pytest -m integration -v
pytest -m e2e -v

# Ejecutar tests con cobertura
pytest --cov=proyecto_j --cov=processing --cov=orchestrator
```

## ğŸ“Š Tipos de Tests

### ğŸ§ª Tests Unitarios (`unit/`)

- **PropÃ³sito**: Probar funciones y mÃ³dulos individuales
- **Alcance**: Funciones especÃ­ficas, clases, mÃ©todos
- **Velocidad**: RÃ¡pidos (< 1 segundo cada uno)
- **Dependencias**: MÃ­nimas, principalmente fixtures

**Ejemplos**:
- Importaciones de mÃ³dulos
- Funciones de cÃ¡lculo estadÃ­stico
- Validadores de datos
- Clasificadores de variables

### ğŸ”— Tests de IntegraciÃ³n (`integration/`)

- **PropÃ³sito**: Probar interacciÃ³n entre mÃ³dulos
- **Alcance**: MÃºltiples componentes trabajando juntos
- **Velocidad**: Moderados (1-10 segundos cada uno)
- **Dependencias**: Fixtures, datos de prueba

**Ejemplos**:
- Pipeline completo de procesamiento
- IntegraciÃ³n con Streamlit
- Sistema de logging
- Procesamiento de datos con validaciÃ³n

### ğŸŒ Tests E2E (`e2e/`)

- **PropÃ³sito**: Probar flujos completos de la aplicaciÃ³n
- **Alcance**: Toda la aplicaciÃ³n desde entrada hasta salida
- **Velocidad**: Lentos (10-60 segundos cada uno)
- **Dependencias**: Datos reales, configuraciÃ³n completa

**Ejemplos**:
- Flujo completo de anÃ¡lisis de datos
- ExportaciÃ³n de reportes
- Consultas en lenguaje natural
- Procesamiento de archivos grandes

## ğŸ“¦ Fixtures

### Datos de Prueba

- `sample_data`: DataFrame pequeÃ±o con datos de ejemplo
- `sample_data_large`: DataFrame grande para tests de rendimiento
- `sample_data_missing`: DataFrame con valores faltantes
- `sample_csv_file`: Archivo CSV temporal
- `sample_excel_file`: Archivo Excel temporal

### ConfiguraciÃ³n

- `test_config`: ConfiguraciÃ³n de prueba
- `temp_dir`: Directorio temporal
- `chile_geographic_data`: Datos geogrÃ¡ficos de Chile
- `expected_outputs`: Salidas esperadas para validaciÃ³n

### Mocks

- `mock_streamlit`: Mock de Streamlit para tests

## ğŸ·ï¸ Marcadores

### Marcadores AutomÃ¡ticos

Los tests se marcan automÃ¡ticamente segÃºn su ubicaciÃ³n:

- `@pytest.mark.unit`: Tests en `unit/`
- `@pytest.mark.integration`: Tests en `integration/`
- `@pytest.mark.e2e`: Tests en `e2e/`

### Marcadores Manuales

- `@pytest.mark.slow`: Tests lentos (performance, datos grandes)
- `@pytest.mark.performance`: Tests de rendimiento

## ğŸ“ˆ Cobertura

### Generar Reporte de Cobertura

```bash
# Con reporte HTML
pytest --cov=proyecto_j --cov=processing --cov=orchestrator --cov-report=html:tests/coverage_html

# Con reporte XML (para CI/CD)
pytest --cov=proyecto_j --cov=processing --cov=orchestrator --cov-report=xml:tests/coverage.xml
```

### Ver Cobertura

- **HTML**: Abrir `tests/coverage_html/index.html`
- **XML**: Usar con herramientas de CI/CD

## ğŸ”§ ConfiguraciÃ³n

### pytest.ini

ConfiguraciÃ³n centralizada de pytest con:
- Marcadores personalizados
- ConfiguraciÃ³n de cobertura
- Filtros de warnings
- Timeouts
- ParalelizaciÃ³n

### conftest.py

Fixtures y configuraciÃ³n comÃºn:
- Datos de prueba
- Mocks
- ConfiguraciÃ³n de paths
- Marcadores automÃ¡ticos

## ğŸ“‹ Convenciones

### Nomenclatura

- Archivos: `test_*.py`
- Clases: `Test*`
- MÃ©todos: `test_*`
- Fixtures: `*_fixture`

### Estructura de Tests

```python
class TestModuleName:
    """Tests para el mÃ³dulo ModuleName"""
    
    def test_specific_functionality(self, fixture_name):
        """Test de funcionalidad especÃ­fica"""
        # Arrange
        # Act
        # Assert
```

### Assertions

- Usar assertions especÃ­ficos de pytest
- Incluir mensajes descriptivos
- Verificar tipos y valores esperados

## ğŸš¨ Troubleshooting

### Problemas Comunes

1. **ImportError**: Verificar que el path del proyecto estÃ¡ configurado
2. **FixtureNotFound**: Verificar que las fixtures estÃ¡n definidas en conftest.py
3. **Timeout**: Aumentar timeout en pytest.ini para tests lentos
4. **MemoryError**: Reducir tamaÃ±o de datos de prueba

### Debugging

```bash
# Ejecutar tests con mÃ¡s informaciÃ³n
pytest -v -s --tb=long

# Ejecutar test especÃ­fico con debug
pytest tests/unit/test_imports.py::TestImports::test_core_imports -v -s
```

## ğŸ”„ Mantenimiento

### Agregar Nuevos Tests

1. Crear archivo en la carpeta apropiada (`unit/`, `integration/`, `e2e/`)
2. Seguir convenciones de nomenclatura
3. Usar fixtures existentes cuando sea posible
4. Agregar marcadores apropiados

### Actualizar Fixtures

1. Modificar `conftest.py`
2. Verificar que todos los tests siguen funcionando
3. Actualizar documentaciÃ³n si es necesario

### Limpiar Tests Legacy

Los tests marcados como "legacy" son versiones anteriores que se mantienen por compatibilidad. Se pueden eliminar cuando:

1. Los nuevos tests cubren la misma funcionalidad
2. Se verifica que no hay regresiones
3. Se actualiza la documentaciÃ³n

## ğŸ“š Recursos

- [DocumentaciÃ³n de pytest](https://docs.pytest.org/)
- [Mejores prÃ¡cticas de testing](https://docs.pytest.org/en/stable/explanation/goodpractices.html)
- [Fixtures avanzadas](https://docs.pytest.org/en/stable/explanation/fixtures.html)
- [Marcadores personalizados](https://docs.pytest.org/en/stable/how-to/mark.html) 